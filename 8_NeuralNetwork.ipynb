{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_mode = 'restricted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Data Takeover](#Data-Takeover)\n",
    "    - [Train/Test Split](#Train/Test-Split)\n",
    "- [Neural Network Implementation](#Neural-Network-Implementation)\n",
    "    - [Performance Measurement](#Performance-Measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Takeover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in DataFrame from chapter [Feature Matrix Generation](./3_FeatureMatrixGeneration.ipynb) as input for processing in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duplicates</th>\n",
       "      <th>coordinate_E_delta</th>\n",
       "      <th>coordinate_N_delta</th>\n",
       "      <th>corporate_full_delta</th>\n",
       "      <th>doi_delta</th>\n",
       "      <th>edition_delta</th>\n",
       "      <th>exactDate_delta</th>\n",
       "      <th>format_postfix_delta</th>\n",
       "      <th>format_prefix_delta</th>\n",
       "      <th>isbn_delta</th>\n",
       "      <th>...</th>\n",
       "      <th>musicid_delta</th>\n",
       "      <th>part_delta</th>\n",
       "      <th>person_100_delta</th>\n",
       "      <th>person_245c_delta</th>\n",
       "      <th>person_700_delta</th>\n",
       "      <th>pubinit_delta</th>\n",
       "      <th>scale_delta</th>\n",
       "      <th>ttlfull_245_delta</th>\n",
       "      <th>ttlfull_246_delta</th>\n",
       "      <th>volumes_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.00000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "      <td>260733.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005649</td>\n",
       "      <td>-0.094006</td>\n",
       "      <td>-0.093732</td>\n",
       "      <td>-0.073601</td>\n",
       "      <td>-0.094386</td>\n",
       "      <td>-0.083392</td>\n",
       "      <td>0.364475</td>\n",
       "      <td>0.430583</td>\n",
       "      <td>0.420806</td>\n",
       "      <td>0.376157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074878</td>\n",
       "      <td>-0.01082</td>\n",
       "      <td>0.201556</td>\n",
       "      <td>0.433603</td>\n",
       "      <td>0.175853</td>\n",
       "      <td>0.188908</td>\n",
       "      <td>-0.093757</td>\n",
       "      <td>0.562582</td>\n",
       "      <td>-0.083317</td>\n",
       "      <td>0.207096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074950</td>\n",
       "      <td>0.034547</td>\n",
       "      <td>0.039364</td>\n",
       "      <td>0.068478</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.059230</td>\n",
       "      <td>0.175742</td>\n",
       "      <td>0.330969</td>\n",
       "      <td>0.493689</td>\n",
       "      <td>0.484414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071869</td>\n",
       "      <td>0.18861</td>\n",
       "      <td>0.361736</td>\n",
       "      <td>0.252787</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>0.293114</td>\n",
       "      <td>0.039281</td>\n",
       "      <td>0.109808</td>\n",
       "      <td>0.069896</td>\n",
       "      <td>0.342763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.10000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.10000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.505947</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.05000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.523228</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.544974</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.05000</td>\n",
       "      <td>0.535227</td>\n",
       "      <td>0.580210</td>\n",
       "      <td>0.529240</td>\n",
       "      <td>0.491522</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.599688</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          duplicates  coordinate_E_delta  coordinate_N_delta  \\\n",
       "count  260733.000000       260733.000000       260733.000000   \n",
       "mean        0.005649           -0.094006           -0.093732   \n",
       "std         0.074950            0.034547            0.039364   \n",
       "min         0.000000           -0.100000           -0.100000   \n",
       "25%         0.000000           -0.100000           -0.100000   \n",
       "50%         0.000000           -0.100000           -0.100000   \n",
       "75%         0.000000           -0.100000           -0.100000   \n",
       "max         1.000000            1.000000            1.000000   \n",
       "\n",
       "       corporate_full_delta      doi_delta  edition_delta  exactDate_delta  \\\n",
       "count         260733.000000  260733.000000  260733.000000    260733.000000   \n",
       "mean              -0.073601      -0.094386      -0.083392         0.364475   \n",
       "std                0.068478       0.020315       0.059230         0.175742   \n",
       "min               -0.100000      -0.100000      -0.100000         0.000000   \n",
       "25%               -0.100000      -0.100000      -0.100000         0.250000   \n",
       "50%               -0.100000      -0.100000      -0.100000         0.312500   \n",
       "75%               -0.050000      -0.100000      -0.050000         0.500000   \n",
       "max                1.000000       1.000000       1.000000         1.000000   \n",
       "\n",
       "       format_postfix_delta  format_prefix_delta     isbn_delta  ...  \\\n",
       "count         260733.000000        260733.000000  260733.000000  ...   \n",
       "mean               0.430583             0.420806       0.376157  ...   \n",
       "std                0.330969             0.493689       0.484414  ...   \n",
       "min                0.000000             0.000000       0.000000  ...   \n",
       "25%                0.111111             0.000000       0.000000  ...   \n",
       "50%                0.428571             0.000000       0.000000  ...   \n",
       "75%                0.428571             1.000000       1.000000  ...   \n",
       "max                1.000000             1.000000       1.000000  ...   \n",
       "\n",
       "       musicid_delta    part_delta  person_100_delta  person_245c_delta  \\\n",
       "count  260733.000000  260733.00000     260733.000000      260733.000000   \n",
       "mean       -0.074878      -0.01082          0.201556           0.433603   \n",
       "std         0.071869       0.18861          0.361736           0.252787   \n",
       "min        -0.100000      -0.10000         -0.100000          -0.100000   \n",
       "25%        -0.100000      -0.10000         -0.050000           0.438095   \n",
       "50%        -0.100000      -0.05000         -0.050000           0.523228   \n",
       "75%        -0.050000      -0.05000          0.535227           0.580210   \n",
       "max         1.000000       1.00000          1.000000           1.000000   \n",
       "\n",
       "       person_700_delta  pubinit_delta    scale_delta  ttlfull_245_delta  \\\n",
       "count     260733.000000  260733.000000  260733.000000      260733.000000   \n",
       "mean           0.175853       0.188908      -0.093757           0.562582   \n",
       "std            0.310796       0.293114       0.039281           0.109808   \n",
       "min           -0.100000      -0.100000      -0.100000           0.000000   \n",
       "25%           -0.050000      -0.050000      -0.100000           0.505947   \n",
       "50%           -0.050000      -0.050000      -0.100000           0.544974   \n",
       "75%            0.529240       0.491522      -0.100000           0.599688   \n",
       "max            1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "       ttlfull_246_delta  volumes_delta  \n",
       "count      260733.000000  260733.000000  \n",
       "mean           -0.083317       0.207096  \n",
       "std             0.069896       0.342763  \n",
       "min            -0.100000      -0.100000  \n",
       "25%            -0.100000      -0.050000  \n",
       "50%            -0.100000       0.000000  \n",
       "75%            -0.100000       0.555556  \n",
       "max             1.000000       1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_goldstandard = './daten_goldstandard'\n",
    "\n",
    "# Restore results so far\n",
    "df_labelled_feature_matrix = pd.read_pickle(os.path.join(path_goldstandard,\n",
    "                                                         'labelled_feature_matrix.pkl'),\n",
    "                                 compression=None)\n",
    "\n",
    "df_attribute_with_sim_feature = pd.read_pickle(os.path.join(\n",
    "    path_goldstandard, 'labelled_feature_matrix_full.pkl'), compression=None\n",
    "                                              )\n",
    "\n",
    "df_labelled_feature_matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of duplicates (1) on uniques (2) in units of [%]\n",
      "0    99.435054\n",
      "1     0.564946\n",
      "Name: duplicates, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Part of duplicates (1) on uniques (2) in units of [%]')\n",
    "print(df_labelled_feature_matrix.duplicates.value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test split will be implemented here as a general function to be called in the models chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.1       , -0.1       , -0.1       , -0.1       , -0.1       ,\n",
       "          0.5       ,  0.42857143,  0.        ,  1.        , -0.1       ,\n",
       "         -0.05      , -0.05      , -0.05      ,  0.50165426, -0.05      ,\n",
       "          0.48593074, -0.1       ,  0.60439973, -0.1       ,  0.        ],\n",
       "        [-0.1       , -0.1       , -0.05      , -0.1       , -0.1       ,\n",
       "          0.        ,  0.42857143,  0.        ,  0.        , -0.1       ,\n",
       "         -0.1       , -0.1       , -0.05      ,  0.54435379, -0.1       ,\n",
       "         -0.05      , -0.1       ,  0.54177001, -0.1       , -0.05      ],\n",
       "        [-0.1       , -0.1       , -0.05      , -0.1       , -0.1       ,\n",
       "          0.        ,  1.        ,  1.        ,  0.        , -0.1       ,\n",
       "         -0.1       , -0.05      , -0.05      ,  0.6020276 ,  0.53663004,\n",
       "          0.49448622, -0.1       ,  0.57046955, -0.1       , -0.05      ],\n",
       "        [-0.1       , -0.1       , -0.1       , -0.1       , -0.1       ,\n",
       "          0.25      ,  1.        ,  1.        ,  1.        , -0.1       ,\n",
       "         -0.1       , -0.1       , -0.05      , -0.05      , -0.1       ,\n",
       "          0.4962963 , -0.1       ,  0.54416035, -0.1       ,  0.        ],\n",
       "        [-0.1       , -0.1       , -0.1       , -0.1       , -0.05      ,\n",
       "          0.625     ,  1.        ,  1.        ,  0.        , -0.1       ,\n",
       "         -0.1       , -0.05      , -0.05      ,  0.51341896, -0.05      ,\n",
       "         -0.05      , -0.1       ,  0.50913242, -0.1       ,  0.        ]]),\n",
       " array([0, 0, 0, 0, 0]),\n",
       " array([130089,  57628, 221651,   3506, 145824]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import classifier_fitting_funcs as cff\n",
    "\n",
    "X_tr, _, X_te, y_tr, _, y_te, idx_tr, _, idx_te = cff.split_feature_target(\n",
    "    df_labelled_feature_matrix, 'train_test')\n",
    "\n",
    "X_tr[:5], y_tr[:5], idx_tr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208586, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_nn(params):\n",
    "    # Input layer\n",
    "    hidden1 = Dense(\n",
    "#        12, # Number of units for hidden layer\n",
    "        params['number_of_hidden1_layers'], # Number of units for hidden layer\n",
    "        input_shape=(X_tr.shape[1],),\n",
    "        activation='relu',\n",
    "        kernel_initializer=VarianceScaling(scale=2.0, seed=0),\n",
    "        kernel_regularizer=l2(params['l2_alpha']),\n",
    "        bias_initializer='zeros'\n",
    "    )\n",
    "\n",
    "    # With dropout layer\n",
    "    dropout = Dropout(\n",
    "        rate=params['dropout_rate'],\n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "    if params['number_of_hidden2_layers'] > 0 :\n",
    "        hidden2 = Dense(\n",
    "    #        12, # Number of units for hidden layer\n",
    "            params['number_of_hidden2_layers'], # Number of units for hidden layer\n",
    "            input_shape=(params['number_of_hidden1_layers'],),\n",
    "            activation='relu',\n",
    "            kernel_initializer=VarianceScaling(scale=2.0, seed=0),\n",
    "            kernel_regularizer=l2(params['l2_alpha']),\n",
    "            bias_initializer='zeros'\n",
    "        )\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(\n",
    "        2,\n",
    "    #    activation='sigmoid', # Bring out 0 or 1 values\n",
    "        activation='softmax', # Bring out 0 or 1 values\n",
    "        kernel_initializer=VarianceScaling(scale=1, seed=0),\n",
    "        kernel_regularizer=l2(params['l2_alpha']),\n",
    "        bias_initializer='zeros'\n",
    "    )\n",
    "\n",
    "    # Create model with sequential API\n",
    "    model = Sequential()\n",
    "    model.add( hidden1 ) # Hidden layer 1\n",
    "    model.add( dropout ) # Dropout\n",
    "    if params['number_of_hidden2_layers'] > 0 :\n",
    "        model.add( hidden2 ) # Hidden layer 2\n",
    "    model.add( output ) # Output layer\n",
    "\n",
    "    # The optimizer is to be SGD\n",
    "    #sgd = SGD(lr=params['sgd_learnrate'])\n",
    "    model_optimizer = Adam(lr=params['sgd_learnrate'])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=model_optimizer,\n",
    "        metrics=['accuracy']\n",
    "    #    metrics=['categorical_accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(history, params):\n",
    "    plt.plot(history.history['accuracy'], label='training accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(r'Bilayer neural network with lr = {} and $\\alpha=${}'.format(\n",
    "        params['sgd_learnrate'], params['l2_alpha'])\n",
    "    )\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylim(0.99, 1.0)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grid parameters are ...\n",
      "l2_alpha [0.0]\n",
      "sgd_learnrate [0.001, 0.002, 0.003]\n",
      "dropout_rate [0.1, 0.2]\n",
      "class_weight [array([ 0.50283981, 88.53395586])]\n",
      "number_of_hidden1_layers [45, 50, 55, 60, 65, 70, 75]\n",
      "number_of_hidden2_layers [0, 45, 50, 55, 60]\n",
      " => Number of combinations : 210\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    " \n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_tr), y_tr)\n",
    "\n",
    "if execution_mode == 'full' :\n",
    "    parameter_dictionary = {\n",
    "        'l2_alpha' : [0.0#, 0.01, 0.05, 0.1, 0.5\n",
    "                     ],\n",
    "        # As soon as a l2_alpha > 0 comes in, the network does not converge anymore.\n",
    "        #  => Go on with l2_alpha = 0, only.\n",
    "        'sgd_learnrate' : [0.001, 0.002, 0.003],\n",
    "        # A learning rate of 0.001 and slightly slower gives good results.\n",
    "        'dropout_rate' : [#0.0, \n",
    "            0.1, 0.2],\n",
    "        'class_weight' : [#None, \n",
    "            class_weights],\n",
    "        'number_of_hidden1_layers' : [#2, 8, 15, 20, 25, 40, \n",
    "            45, 50, 55, 60, 65, 70, 75],\n",
    "        # A number of hidden layers of 2 is too small. The bigger the number of hidden layers,\n",
    "        #  the slower the learning rate. There are 20 features.\n",
    "        'number_of_hidden2_layers' : [0, 45, 50, 55, 60]\n",
    "    }\n",
    "elif execution_mode == 'restricted' :\n",
    "    parameter_dictionary = {\n",
    "        'l2_alpha' : [0.0],\n",
    "        'sgd_learnrate' : [0.002],\n",
    "        'dropout_rate' : [0.1],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'number_of_hidden1_layers' : [40, 60],\n",
    "        'number_of_hidden2_layers' : [0, 70]\n",
    "    }\n",
    "\n",
    "# Grid of values for nn with 1 hidden layer\n",
    "grid = cff.generate_parameter_grid(parameter_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andreas/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Fitting with parameters {'class_weight': None, 'dropout_rate': 0.1, 'l2_alpha': 0.0, 'number_of_hidden1_layers': 45, 'number_of_hidden2_layers': 0, 'sgd_learnrate': 0.001}\n",
      "WARNING:tensorflow:From /Users/andreas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2c64663c0455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Save accuracy on test set\n",
    "no_last = 5 # Take the last 5 due to the upwards shape of the curve at its end\n",
    "test_scores = []\n",
    "\n",
    "# Parameters for fitting with batches and epochs\n",
    "epochs = 100\n",
    "batch_size = 320 # Default batch_size = 32\n",
    "# Playing around with batch sizes of [3, 30, None=32, 320, 3200] shows the effect\n",
    "#  that the bigger the size, the faster the calculation performance,\n",
    "#  ... the worse the convergence. => Batch size has effect of learning rate.\n",
    "# 320 seems to be the perfect value.\n",
    "\n",
    "for params_dict in grid :\n",
    "\n",
    "    model = build_and_compile_nn(params_dict)\n",
    "    \n",
    "    print('Fitting with parameters', params_dict)\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        x=X_tr, y=to_categorical(y_tr),\n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        validation_data=(X_te, to_categorical(y_te)),\n",
    "        class_weight=params_dict['class_weight']\n",
    "        , verbose=0\n",
    "    )\n",
    "\n",
    "    # Save accuracy on train set and validation set\n",
    "    params_dict['accuracy_tr'] = np.mean(history.history['accuracy'][-no_last:])\n",
    "    params_dict['accuracy_val'] = np.mean(history.history['val_accuracy'][-no_last:])\n",
    "    params_dict['log_accuracy_tr'] = np.log(1-np.mean(history.history['accuracy'][-no_last:]))\n",
    "    params_dict['log_accuracy_val'] = np.log(1-np.mean(history.history['val_accuracy'][-no_last:]))\n",
    "\n",
    "    print(' => validation score {:.3f}%'.format(100*params_dict['accuracy_val']))\n",
    "    # Save result\n",
    "    test_scores.append(params_dict)\n",
    "    \n",
    "    plot_result(history, params_dict)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "# Save measured accuracies\n",
    "df_test_scores_nn = pd.DataFrame(test_scores).sort_values('accuracy_val', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves of the validation accuracy above show the behaviour of approaching their constant maximum value only very slowly. A very high number of epochs is needed to reach the maximum value of the validation accuracy. Some simulation have been done in the course of the project with a number of epochs of 500. Even with this high number of epochs, there could still be seen a slight slope in the validation accurace, indicating that the model was still learning and improving. The validation accuracy has never surpassed a value of 99.93%, though. The latter observation led to the conviction that the accuracy of the model would not be increased significantly, increasing the number of epochs even over 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", len(grid))\n",
    "\n",
    "pd.DataFrame(test_scores).sort_values('accuracy_val', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = cff.get_best_parameters(test_scores, parameter_dictionary)\n",
    "\n",
    "model_best = build_and_compile_nn(best_params)\n",
    "\n",
    "# Check Model configuration\n",
    "model_best.get_config()\n",
    "\n",
    "# Parameters for fitting with batches and epochs\n",
    "epochs = 300\n",
    "batch_size = 320\n",
    "\n",
    "# Fit the model\n",
    "history_best = model_best.fit(\n",
    "    x=X_tr, y=to_categorical(y_tr),\n",
    "    batch_size=batch_size, epochs=epochs,\n",
    "    validation_data=(X_te, to_categorical(y_te))\n",
    "    , verbose=0\n",
    ")\n",
    "y_pred = model_best.predict_classes(X_te)\n",
    "\n",
    "plot_result(history_best, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "path_model_graphics = './documentation'\n",
    "model_png = os.path.join(path_model_graphics,'model.png')\n",
    "\n",
    "plot_model(model_best, show_shapes=True, dpi=72, to_file=model_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_last = 5 # I take the last 5 due to the upwards shape of the curve at its end\n",
    "print('Mean last {:d} validation accuracy : {:.3f}'.format(\n",
    "    no_last, np.mean(history_best.history['val_accuracy'][-no_last:])\n",
    "))\n",
    "\n",
    "print('Neural network accuracy (test set): {:.3f}'.format(\n",
    "    model.evaluate(X_te, to_categorical(y_te),\n",
    "                   verbose=0)[1] # Loss is at index=0, accuracy at index=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from keras.metrics import accuracy\n",
    "\n",
    "#print('Score {:.1f}%'.format(100*accuracy_score(X_te, y_te)))\n",
    "print('Area under the curve {:.1f}% - accuracy {:.1f}% - precision {:.1f}% - recall {:.1f}%'.format(100*roc_auc_score(y_te, y_pred),\n",
    "                100*accuracy_score(y_te, y_pred),\n",
    "                100*precision_score(y_te, y_pred),\n",
    "                100*recall_score(y_te, y_pred)\n",
    "               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import results_analysis_funcs as raf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_feature_base_full_te = df_attribute_with_sim_feature.iloc[idx_te]\n",
    "df_feature_base_full_tr = df_attribute_with_sim_feature.iloc[idx_tr]\n",
    "\n",
    "# Extend display to number of columns of DataFrame\n",
    "pd.options.display.max_columns = len(df_feature_base_full_te.columns)\n",
    "\n",
    "df_feature_base_full_te.sort_index().sample(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import results_saving_funcs as rsf\n",
    "\n",
    "idx = {}\n",
    "idx['true_predicted_uniques'], idx['true_predicted_duplicates'], idx['false_predicted_uniques'], idx['false_predicted_duplicates'] = raf.get_confusion_matrix_indices(y_te, y_pred)\n",
    "\n",
    "wrong_prediction_groups = ['false_predicted_uniques', 'false_predicted_duplicates']\n",
    "\n",
    "for i in wrong_prediction_groups :\n",
    "    rsf.add_wrong_predictions(path_goldstandard, \n",
    "                              model_best, i, df_feature_base_full_te.loc[idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Handover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf.add_result_to_results(path_goldstandard, df_test_scores_nn, model_best, X_te, y_te, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

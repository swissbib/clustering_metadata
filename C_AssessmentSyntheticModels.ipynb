{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment of Models Trained with Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models with the help of synthetic data results in classifiers which exhibit an extraordinary high performance as can be seen in chapter [Overview and Summary](./0_OverviewSummary.ipynb). This measured performance may be due to two reasons.\n",
    "\n",
    "1. The data synthesis may have generated easy data to classify. More precisely stated, the synthetic data may be easier to classify than Swissbib's original data. When tested, the models do not experience any difficulty with the synthetic data. As their amount is considerably bigger than the amount of Swissbib's original data, the statistics improves.\n",
    "1. The models trained with synthetic data have seen more pairs of duplicates during their training process. The higher experience of the models results in a better classification ability. This argument would be independent of the ratio of synthetic and pure Swissbib data.\n",
    "\n",
    "Which of the two explanations is valid and an assessment of the quality of the model fitted with the synthetic data of this capstone project, shall be investigated in this appendix. To do so, the steps are as follows.\n",
    "\n",
    "1. Synthetic data is generated for training and testing the models.\n",
    "1. The models to be investigated are fitted with this data. This appendix concentrates on two models, only. The models of the Ensemble family will be chosen.\n",
    "1. The data generation is redone, in this step with pure Swissbib data in the absence of any artificial synthetic data.\n",
    "1. The models under consideration are fitted with this pure training data.\n",
    "1. The performances for all estimators are assessed with the help of the testing data split off from the synthetic data and separately with the help of the testing data split off from the pure Swissbib data.\n",
    "\n",
    "This process is reflected in the table of contents below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Table of Contents\n",
    "\n",
    "- [Generation of Synthetic Data](#Generation-of-Synthetic-Data)\n",
    "- [Models' Training with Synthetic Data](#Models'-Training-with-Synthetic-Data)\n",
    "    - [Decision Tree Classifier Training](#Decision-Tree-Classifier-Training)\n",
    "    - [Random Forests Classifier Training](#Random-Forests-Classifier-Training)\n",
    "- [Generation of Pure Swissbib Data](#Generation-of-Pure-Swissbib-Data)\n",
    "- [Models' Training with Pure Swissbib Data](#Models'-Training-with-Pure-Swissbib-Data)\n",
    "- [Performance Measurements](#Performance-Measurements)\n",
    "    - [Decision Tree Classifier Performance](#Decision-Tree-Classifier-Performance)\n",
    "    - [Random Forests Classifier Performance](#Random-Forests-Classifier-Performance)\n",
    "- [Summary and Assessment](#Summary-and-Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wanted synthetic data can be produced easily calling the first notebooks of the capstone project, reusing code snippets of chapter [Overview and Summary](./0_OverviewSummary.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import results_saving_funcs as rsf\n",
    "import pandas as pd\n",
    "\n",
    "def get_training_testing_data(nbs, run):\n",
    "    path_results = './results'\n",
    "    path_goldstandard = './daten_goldstandard'\n",
    "\n",
    "    rsf.run_notebooks(nbs, runtime_param_dict, run, path_results)\n",
    "\n",
    "    #Â Restore results so far\n",
    "    df = pd.read_pickle(os.path.join(path_goldstandard, 'labelled_feature_matrix.pkl'), compression=None)\n",
    "    display(df.head())\n",
    "\n",
    "    print('Part of duplicates (1) and uniques (0) in units of [%]')\n",
    "    print(df.duplicates.value_counts())\n",
    "    print('Part of duplicates (1) and uniques (0) in units of [%]')\n",
    "    print(round(df.duplicates.value_counts(normalize=True)*100, 2))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for run : \n",
      " {'em': 'full', 'os': 20, 'mr': 0.2, 'dsn': 1, 'dsw': 1, 'fa': 1.0, 'me': 'added_u', 'sn': True}\n"
     ]
    }
   ],
   "source": [
    "# Generate dictionary for parameter handover\n",
    "runtime_param_dict = {\n",
    "    'em' : 'full' #execution_mode\n",
    "    , 'os' : 20 # oversampling\n",
    "    , 'mr' : 0.2 # modification_ratio\n",
    "    , 'dsn' : 1 # ğšœğšŠğš–ğš™ğš•ğš’ğš—ğš_ğšğš›ğšŠğšŒğšğš’ğš˜ğš—_ğš—ğš›ğšğš‹\n",
    "    , 'dsw' : 1 # ğšœğšŠğš–ğš™ğš•ğš’ğš—ğš_ğšğš›ğšŠğšŒğšğš’ğš˜ğš—_ğš›ğšğš‹\n",
    "    , 'fa' : 1.0 #Â factor\n",
    "    , 'me' : 'added_u' # mode_exactDate\n",
    "    , 'sn' : True #Â strip_number_digits\n",
    "}\n",
    "\n",
    "#Â Let's have a look at the predefined parameters\n",
    "print('Parameters for run : \\n', runtime_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing notebook 2_GoldstandardDataPreparation.ipynb\n",
      "Executing notebook 3_DataSynthesizing.ipynb\n",
      "Executing notebook 4_FeatureMatrixGeneration.ipynb\n",
      "Executing notebook 5_FeatureDiscussionDummyBaseline.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinate_E_delta</th>\n",
       "      <th>coordinate_N_delta</th>\n",
       "      <th>corporate_full_delta</th>\n",
       "      <th>doi_delta</th>\n",
       "      <th>edition_delta</th>\n",
       "      <th>exactDate_delta</th>\n",
       "      <th>format_prefix_delta</th>\n",
       "      <th>format_postfix_delta</th>\n",
       "      <th>isbn_delta</th>\n",
       "      <th>ismn_delta</th>\n",
       "      <th>...</th>\n",
       "      <th>part_delta</th>\n",
       "      <th>person_100_delta</th>\n",
       "      <th>person_700_delta</th>\n",
       "      <th>person_245c_delta</th>\n",
       "      <th>pubinit_delta</th>\n",
       "      <th>scale_delta</th>\n",
       "      <th>ttlfull_245_delta</th>\n",
       "      <th>ttlfull_246_delta</th>\n",
       "      <th>volumes_delta</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.697740</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coordinate_E_delta  coordinate_N_delta  corporate_full_delta  doi_delta  \\\n",
       "0                -1.0                -1.0                  -1.0       -1.0   \n",
       "1                -1.0                -1.0                  -1.0       -1.0   \n",
       "2                -1.0                -1.0                  -1.0       -1.0   \n",
       "3                -1.0                -1.0                  -1.0       -1.0   \n",
       "4                -1.0                -1.0                  -1.0       -1.0   \n",
       "\n",
       "   edition_delta  exactDate_delta  format_prefix_delta  format_postfix_delta  \\\n",
       "0           -1.0             0.75                  1.0                   1.0   \n",
       "1           -1.0             0.75                  1.0                   1.0   \n",
       "2           -1.0             0.75                  1.0                   1.0   \n",
       "3           -1.0             0.75                  1.0                   1.0   \n",
       "4           -1.0             0.75                  1.0                   1.0   \n",
       "\n",
       "   isbn_delta  ismn_delta  ...  part_delta  person_100_delta  \\\n",
       "0         1.0        -1.0  ...         1.0               1.0   \n",
       "1         1.0        -1.0  ...         1.0               1.0   \n",
       "2         1.0        -1.0  ...         1.0               1.0   \n",
       "3         1.0        -1.0  ...         1.0               1.0   \n",
       "4         1.0        -1.0  ...         1.0               1.0   \n",
       "\n",
       "   person_700_delta  person_245c_delta  pubinit_delta  scale_delta  \\\n",
       "0               1.0           1.000000       1.000000         -1.0   \n",
       "1              -0.5           0.818905       0.848485         -1.0   \n",
       "2              -0.5           0.697740       0.848485         -1.0   \n",
       "3              -0.5           0.818905       0.848485         -1.0   \n",
       "4              -1.0           1.000000       1.000000         -1.0   \n",
       "\n",
       "   ttlfull_245_delta  ttlfull_246_delta  volumes_delta  duplicates  \n",
       "0           1.000000               -1.0            1.0           1  \n",
       "1           0.787879               -1.0            1.0           1  \n",
       "2           1.000000               -1.0            1.0           1  \n",
       "3           0.787879               -1.0            1.0           1  \n",
       "4           1.000000               -1.0            1.0           1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of duplicates (1) and uniques (0) in units of [%]\n",
      "0    257955\n",
      "1     67158\n",
      "Name: duplicates, dtype: int64\n",
      "Part of duplicates (1) and uniques (0) in units of [%]\n",
      "0    79.34\n",
      "1    20.66\n",
      "Name: duplicates, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Â Determine all relevant notebooks, omit Overview Summary and Appendixes\n",
    "notebook = ! ls [2-5]_* | grep .ipynb\n",
    "\n",
    "df_labelled_feature_matrix = get_training_testing_data(notebook, 'c0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models' Training with Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wanted data has been produced, above. Now, the two models for assessment can be calculated. Only one estimator will be calculated for each model. The model parameters remain constant for the models due to reproduceability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first relevant model is the Decision Tree Classifier fitted with cross-validation. The process has been copied out of chapter [Decision Tree Model](./6_DecisionTreeModel.ipynb) and will not be commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import classifier_fitting_funcs as cff\n",
    "\n",
    "def fit_dtcv(df):\n",
    "    # Training and testing data matrices\n",
    "    X_tr, _, X_te, y_tr, _, y_te, idx_tr, _, idx_te = cff.split_feature_target(df, 'train_test')\n",
    "\n",
    "    print(X_tr.shape, y_tr.shape, X_te.shape, y_te.shape)\n",
    "    print('The test data set holds {:d} records of uniques and {:d} records of duplicates.'.format(\n",
    "        len(y_te[y_te==0]), len(y_te[y_te==1])))\n",
    "\n",
    "    # Find best parameters of Decision Tree\n",
    "    parameters_dtcv = {\n",
    "        'max_depth' : [20], # Number of features\n",
    "        'criterion' : ['gini'], # Criterion for best estimator out of tuning in chapter 0\n",
    "        'class_weight' : ['balanced'] # Class Weight for best estimator out of tuning in chapter 0\n",
    "    }\n",
    "    #Â Grid of values\n",
    "    grid = cff.generate_parameter_grid(parameters_dtcv)\n",
    "\n",
    "    #Â Create cross-validation object with DecisionTreeClassifer\n",
    "    grid_cv = GridSearchCV(DecisionTreeClassifier(random_state=0),\n",
    "                           param_grid = parameters_dtcv, cv=5\n",
    "                           , verbose=1\n",
    "                          )\n",
    "    # Fit estimator\n",
    "    grid_cv.fit(X_tr, y_tr)\n",
    "    dtcv_best_synt = grid_cv.best_estimator_\n",
    "    print(dtcv_best_synt)\n",
    "\n",
    "    # Predict with best estimator\n",
    "    return dtcv_best_synt, X_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260090, 20) (260090,) (65023, 20) (65023,)\n",
      "The test data set holds 51591 records of uniques and 13432 records of duplicates.\n",
      "The grid parameters are ...\n",
      "max_depth [20]\n",
      "criterion ['gini']\n",
      "class_weight ['balanced']\n",
      " => Number of combinations : 1\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
      "                       max_depth=20, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=0, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "dtcv_synt, X_te_dtcv_synt, y_te_dtcv_synt = fit_dtcv(df_labelled_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second relevant model for comparison is the Random Forests Classifier, the best classifier in general, cp. [Overview and Summary](./0_OverviewSummary.ipynb). The process, has been copied out of chapter [Decision Tree Model](./6_DecisionTreeModel.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit_rf(df):\n",
    "    # Training and testing data matrices\n",
    "    X_tr, X_val, X_te, y_tr, y_val, y_te, idx_tr, id_val, idx_te = cff.split_feature_target(df, 'train_validation_test')\n",
    "\n",
    "    print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape, X_te.shape, y_te.shape)\n",
    "    print('The test data set holds {:d} records of uniques and {:d} records of duplicates.'.format(\n",
    "        len(y_te[y_te==0]), len(y_te[y_te==1])))\n",
    "\n",
    "    parameters_rf = {\n",
    "        'n_estimators' : 100,\n",
    "        'max_depth' : 19,\n",
    "        'class_weight' : None\n",
    "    }\n",
    "\n",
    "    # Create a decision tree\n",
    "    rf_best = RandomForestClassifier(n_estimators=parameters_rf['n_estimators'],\n",
    "                                     max_depth=parameters_rf['max_depth'],\n",
    "                                     class_weight=parameters_rf['class_weight'],\n",
    "                                     random_state=0\n",
    "                                    )\n",
    "    # Fit estimator\n",
    "    rf_best.fit(X_tr, y_tr)\n",
    "    print(rf_best)\n",
    "\n",
    "    # Predict with best estimator\n",
    "    return rf_best, X_te, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208072, 20) (208072,) (52018, 20) (52018,) (65023, 20) (65023,)\n",
      "The test data set holds 51591 records of uniques and 13432 records of duplicates.\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=19, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "rf_synt, X_te_rf_synt, y_te_rf_synt = fit_rf(df_labelled_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of Pure Swissbib Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure Swissbib data can be generated calling the first notebooks of the capstone project, the same way as above with different runtime parameters, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for run : \n",
      " {'em': 'full', 'os': 0, 'mr': 0.2, 'dsn': 1, 'dsw': 1, 'fa': 1.0, 'me': 'added_u', 'sn': True}\n"
     ]
    }
   ],
   "source": [
    "# Generate dictionary for parameter handover\n",
    "runtime_param_dict = {\n",
    "    'em' : 'full' #execution_mode\n",
    "    , 'os' : 0 # oversampling\n",
    "    , 'mr' : 0.2 # modification_ratio\n",
    "    , 'dsn' : 1 # ğšœğšŠğš–ğš™ğš•ğš’ğš—ğš_ğšğš›ğšŠğšŒğšğš’ğš˜ğš—_ğš—ğš›ğšğš‹\n",
    "    , 'dsw' : 1 # ğšœğšŠğš–ğš™ğš•ğš’ğš—ğš_ğšğš›ğšŠğšŒğšğš’ğš˜ğš—_ğš›ğšğš‹\n",
    "    , 'fa' : 1.0 #Â factor\n",
    "    , 'me' : 'added_u' # mode_exactDate\n",
    "    , 'sn' : True #Â strip_number_digits\n",
    "}\n",
    "\n",
    "#Â Let's have a look at the predefined parameters\n",
    "print('Parameters for run : \\n', runtime_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing notebook 2_GoldstandardDataPreparation.ipynb\n",
      "Executing notebook 3_DataSynthesizing.ipynb\n",
      "Executing notebook 4_FeatureMatrixGeneration.ipynb\n",
      "Executing notebook 5_FeatureDiscussionDummyBaseline.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinate_E_delta</th>\n",
       "      <th>coordinate_N_delta</th>\n",
       "      <th>corporate_full_delta</th>\n",
       "      <th>doi_delta</th>\n",
       "      <th>edition_delta</th>\n",
       "      <th>exactDate_delta</th>\n",
       "      <th>format_prefix_delta</th>\n",
       "      <th>format_postfix_delta</th>\n",
       "      <th>isbn_delta</th>\n",
       "      <th>ismn_delta</th>\n",
       "      <th>...</th>\n",
       "      <th>part_delta</th>\n",
       "      <th>person_100_delta</th>\n",
       "      <th>person_700_delta</th>\n",
       "      <th>person_245c_delta</th>\n",
       "      <th>pubinit_delta</th>\n",
       "      <th>scale_delta</th>\n",
       "      <th>ttlfull_245_delta</th>\n",
       "      <th>ttlfull_246_delta</th>\n",
       "      <th>volumes_delta</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.697740</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coordinate_E_delta  coordinate_N_delta  corporate_full_delta  doi_delta  \\\n",
       "0                -1.0                -1.0                  -1.0       -1.0   \n",
       "1                -1.0                -1.0                  -1.0       -1.0   \n",
       "2                -1.0                -1.0                  -1.0       -1.0   \n",
       "3                -1.0                -1.0                  -1.0       -1.0   \n",
       "4                -1.0                -1.0                  -1.0       -1.0   \n",
       "\n",
       "   edition_delta  exactDate_delta  format_prefix_delta  format_postfix_delta  \\\n",
       "0           -1.0             0.75                  1.0                   1.0   \n",
       "1           -1.0             0.75                  1.0                   1.0   \n",
       "2           -1.0             0.75                  1.0                   1.0   \n",
       "3           -1.0             0.75                  1.0                   1.0   \n",
       "4           -1.0             0.75                  1.0                   1.0   \n",
       "\n",
       "   isbn_delta  ismn_delta  ...  part_delta  person_100_delta  \\\n",
       "0         1.0        -1.0  ...         1.0               1.0   \n",
       "1         1.0        -1.0  ...         1.0               1.0   \n",
       "2         1.0        -1.0  ...         1.0               1.0   \n",
       "3         1.0        -1.0  ...         1.0               1.0   \n",
       "4         1.0        -1.0  ...         1.0               1.0   \n",
       "\n",
       "   person_700_delta  person_245c_delta  pubinit_delta  scale_delta  \\\n",
       "0               1.0           1.000000       1.000000         -1.0   \n",
       "1              -0.5           0.818905       0.848485         -1.0   \n",
       "2              -0.5           0.697740       0.848485         -1.0   \n",
       "3              -0.5           0.818905       0.848485         -1.0   \n",
       "4              -1.0           1.000000       1.000000         -1.0   \n",
       "\n",
       "   ttlfull_245_delta  ttlfull_246_delta  volumes_delta  duplicates  \n",
       "0           1.000000               -1.0            1.0           1  \n",
       "1           0.787879               -1.0            1.0           1  \n",
       "2           1.000000               -1.0            1.0           1  \n",
       "3           0.787879               -1.0            1.0           1  \n",
       "4           1.000000               -1.0            1.0           1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of duplicates (1) and uniques (0) in units of [%]\n",
      "0    257955\n",
      "1      1473\n",
      "Name: duplicates, dtype: int64\n",
      "Part of duplicates (1) and uniques (0) in units of [%]\n",
      "0    99.43\n",
      "1     0.57\n",
      "Name: duplicates, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_labelled_feature_matrix = get_training_testing_data(notebook, 'c1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models' Training with Pure Swissbib Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the functions of the last section, the models' predictions are super short in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207542, 20) (207542,) (51886, 20) (51886,)\n",
      "The test data set holds 51591 records of uniques and 295 records of duplicates.\n",
      "The grid parameters are ...\n",
      "max_depth [20]\n",
      "criterion ['gini']\n",
      "class_weight ['balanced']\n",
      " => Number of combinations : 1\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight='balanced', criterion='gini',\n",
      "                       max_depth=20, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=0, splitter='best')\n",
      "(166033, 20) (166033,) (41509, 20) (41509,) (51886, 20) (51886,)\n",
      "The test data set holds 51591 records of uniques and 295 records of duplicates.\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=19, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "dtcv_pure, X_te_dtcv_pure, y_te_dtcv_pure = fit_dtcv(df_labelled_feature_matrix)\n",
    "rf_pure, X_te_rf_pure, y_te_rf_pure = fit_rf(df_labelled_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the comparison of the performance can be done with the help of the confusion matrix and its characteristic numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def print_score_values(y_te, y_pred):\n",
    "    print('Area under the curve {:.3f}% - accuracy {:.3f}% - precision {:.3f}% - recall {:.3f}%'.format(\n",
    "        100*roc_auc_score(y_te, y_pred),\n",
    "                    100*accuracy_score(y_te, y_pred),\n",
    "                    100*precision_score(y_te, y_pred),\n",
    "                    100*recall_score(y_te, y_pred)\n",
    "                   ))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following predictions, several combinations can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dtcv_synt_synt = dtcv_synt.predict(X_te_dtcv_synt)\n",
    "y_pred_dtcv_synt_pure = dtcv_synt.predict(X_te_dtcv_pure)\n",
    "y_pred_dtcv_pure_synt = dtcv_pure.predict(X_te_dtcv_synt)\n",
    "y_pred_dtcv_pure_pure = dtcv_pure.predict(X_te_dtcv_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Pure train data, pure test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 98.288% - accuracy 99.946% - precision 94.059% - recall 96.610%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51573,    18],\n",
       "       [   10,   285]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print_score_values(y_te_dtcv_pure, y_pred_dtcv_pure_pure)\n",
    "confusion_matrix(y_te_dtcv_pure, y_pred_dtcv_pure_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original situation and the scoring is the experienced scoring of chapter [Overview and Summary](./0_OverviewSummary.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Synthetic train data, synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 99.856% - accuracy 99.898% - precision 99.725% - recall 99.784%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51554,    37],\n",
       "       [   29, 13403]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print_score_values(y_te_dtcv_synt, y_pred_dtcv_synt_synt)\n",
    "confusion_matrix(y_te_dtcv_synt, y_pred_dtcv_synt_synt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chapter [Overview and Summary](./0_OverviewSummary.ipynb), this combination has been shown as well. Compared with the first constellation as a baseline, this case can be described as in the following list. Some of the statements below may be equivalent.\n",
    "\n",
    "- The amount of records of pairs of duplicates with a total of 13,432 in the data set for testing, is by two orders of magnitude bigger than the amount of duplicates in the first case with pure test data.\n",
    "- The accuracy is lower for this combination here, compared to the combination above.\n",
    "- The score values for precision and recall are significantly higher. They reach a scoring value greater than 99.5% each.\n",
    "- The ratio of false predictions is lower in this second combination compared to the first one.\n",
    "- The total of false predictions for this constellation is higher for the case here, than the total of false predictions for the constallation of the situation above.\n",
    "- The area under the curve is higher here compared to above.\n",
    "- The number of correctly predicted records of pairs of uniques is lower in this combination compared to the same number above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Synthetic train data, pure test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 99.795% - accuracy 99.927% - precision 88.822% - recall 99.661%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51554,    37],\n",
       "       [    1,   294]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_score_values(y_te_dtcv_pure, y_pred_dtcv_synt_pure)\n",
    "confusion_matrix(y_te_dtcv_pure, y_pred_dtcv_synt_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a new constellation and its result is interesting.\n",
    "\n",
    "- The amount of records of pairs of duplicates is with a total of 295 exactly the same as in the first constellation. Only two records of duplicates have been wrongly predicted. The model's power for identifying duplicates seems to have increased for models trained with synthetic data.\n",
    "- The accuracy is at about the same level as for the first case. This observation, together with the first item is equivalent to the observation that more records of uniques have been classified wrongly by this model compared to the first model.\n",
    "- The score value for recall is significantly better for this case, compared to the first one. It is at about the same level as for the second case, above. The high amount of wrongly predicted uniques is expressed in a significantly lower precision score.\n",
    "- The ratio of false predictions with a total of 30 is at about the same level as for the first model with a total of 28.\n",
    "- The area under the curve is higher for this case than for the first combination. It is at about the same value as for the second situation above.\n",
    "- The number of correctly predicted records of pairs of uniques is exactly the same in this combination as in the case of the model trained with synthetic data and tested with synthetic data.\n",
    "\n",
    "**Summary** Training a model with the synthetic data, generated in chapter [Data Synthesizing](./3_DataSynthesizing.ipynb) leads to models that can classify duplicates better at the cost of a decreased ability of identifying uniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Pure train data, synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 85.565% - accuracy 94.016% - precision 99.812% - recall 71.166%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51573,    18],\n",
       "       [ 3873,  9559]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_score_values(y_te_dtcv_synt, y_pred_dtcv_pure_synt)\n",
    "confusion_matrix(y_te_dtcv_synt, y_pred_dtcv_pure_synt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This combination is a countercheck to the second combination above.\n",
    "\n",
    "- The precision score is the highest for all of the four compared combinations. The root cause of this high precision is due to the high total amount of duplicates that have been identified correctly by the model.\n",
    "- The total of 18 wrongly predicted uniques is exactly the same in this combination as in the case of the model trained with synthetic data and tested with synthetic data.\n",
    "- With a total of 3,833, more than one fourth of the records of duplicates has been wrongly classified as uniques. The recall is below 75%.\n",
    "- The area under the curve is lowest for this constellation.\n",
    "\n",
    "**Summary** For models trained with pure Swissbib data, the synthetic data of this capstone project is to a ratio of more than one fourth wrongly identified as uniques. The root cause for this bad performance must be the way of data synthesis. The implementation of chapter [Data Synthesizing](./3_DataSynthesizing.ipynb) seems to produce data that resembles more pairs of distinct records for a model trained exclusively with Swissbib's data reality. Swissbib's data reality seems to be badly copied by the implemented synthesis logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, let's keep in mind the result of the third constellation confirms that models trained with synthetic data. The artificial data seem to generate a better ability for identifying duplicates even for Swissbib's exclusive data reality. So the bad data do have a positive effect on the models when used for training, improving them on identifying duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Random Forests Classifier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same comparisons can be done for the Random Forests Classifier. This classifier is the winner of all classifiers compared in chapter [Overview and Summary](./0_OverviewSummary.ipynb). Therefore, the results below are of big interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_synt_synt = rf_synt.predict(X_te_rf_synt)\n",
    "y_pred_rf_synt_pure = rf_synt.predict(X_te_rf_pure)\n",
    "y_pred_rf_pure_synt = rf_pure.predict(X_te_rf_synt)\n",
    "y_pred_rf_pure_pure = rf_pure.predict(X_te_rf_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Pure train data, pure test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 97.613% - accuracy 99.944% - precision 94.932% - recall 95.254%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51576,    15],\n",
       "       [   14,   281]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_score_values(y_te_rf_pure, y_pred_rf_pure_pure)\n",
    "confusion_matrix(y_te_rf_pure, y_pred_rf_pure_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point is qualitatively the same for this classifier as for the Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Synthetic train data, synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 99.886% - accuracy 99.920% - precision 99.784% - recall 99.829%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51562,    29],\n",
       "       [   23, 13409]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_score_values(y_te_rf_synt, y_pred_rf_synt_synt)\n",
    "confusion_matrix(y_te_rf_synt, y_pred_rf_synt_synt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this constellation, it becomes visible that the Random Forests Classifier shows generally a better performance than the Decision Tree Classifier. The area under the curve is unbeatable, all scores show maximum values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Synthetic train data, pure test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 99.124% - accuracy 99.934% - precision 90.909% - recall 98.305%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51562,    29],\n",
       "       [    5,   290]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_score_values(y_te_rf_pure, y_pred_rf_synt_pure)\n",
    "confusion_matrix(y_te_rf_pure, y_pred_rf_synt_pure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statements for the Decision Tree Classifier can be repeated on a quality level for this classifier. The synthetic data improves the estimator's abilities of classifying correctly records of duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Pure train data, synthetic test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the curve 93.188% - accuracy 97.169% - precision 99.871% - recall 86.406%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51576,    15],\n",
       "       [ 1826, 11606]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_score_values(y_te_rf_synt, y_pred_rf_pure_synt)\n",
    "confusion_matrix(y_te_rf_synt, y_pred_rf_pure_synt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting about the recall of this constellation is its height which is more than 10 percent points bigger than the recall of the corresponding constellation for the Decision Tree Classifier. The Random Forests Classifier is the best classifier, even when tested with synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Summary and Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that the synthetic data of chapter [Data Synthesizing](./3_DataSynthesizing.ipynb) may improve the models slightly when used for training. The synthetic data influence the ability of the models positively in identifying duplicates of Swissbib's pure data reality.\n",
    "\n",
    "On the other hand, the performance measurement show that the implemented data synthesis does not produce data that reflect Swissbib's data reality very precisely. Therefore, only a part of the scoring measures are influenced positively when training the models with synthetic data of chapter [Data Synthesizing](./3_DataSynthesizing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

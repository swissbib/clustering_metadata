{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diskussionspunkte für Andreas und für Silvia zum Mitlesen sowie Nach und Nach \"Hineinkommen\" in die Python Syntax und deren Anwendung im Notebook\n",
    "- in 0_overview_summary\n",
    "1. **Runs execution**\n",
    "\n",
    "warum versuchst Du die Bibliothek nbparameterwise zu installieren?\n",
    "Wir haben diese in requirements.txt Dort sogar mit einer fixen Version. Bin mir nicht sicher, ob\n",
    "#! pip install nbparameterise\n",
    "im Jupyter notebook zu Versionskonflikten führen kann\n",
    "\n",
    "=> AJ : Mit dem Hash-Zeichen kommentiere ich die Zeile aus, sie wird ohne Aktion übersprungen. Damit ist die Zeile eine Altlast der ersten Version ohne requirements.txt. Du findest noch weitere solche pip install-Befehle. So weiss ich im Nachhinein, wo ich die library einbinden würde.\n",
    "\n",
    "2. **Nutzung der Helper Funktionen in den python scripten**\n",
    "\n",
    "classifier_fitting_funcs.py  \n",
    "data_analysis_funcs.py  \n",
    "data_preparation_funcs.py  \n",
    "modify_data_funcs.py  \n",
    "results_analysis_funcs.py  \n",
    "results_saving_funcs.py  \n",
    "\n",
    "Vielleicht wäre es schöner, diese in ein eigenes repository als Module auszulagern (in Github oder Gitlab- Gitlab habe ich nicht getestet) und dann im notebook über requirements.txt als dependency zu definieren.\n",
    "\n",
    "=> AJ : Bin ich offen. Allerdings sind diese libraries extrem proprietär und für niemand sonst von Nutzen. Auch erhebe ich keinen Anspruch darauf, in irgendeiner Weise allgemeingültigen Nutzen mit ihnen zu generieren.\n",
    "Ich bin offen für Neues, wenn du mich den Nutzen erkennen lässt.\n",
    "\n",
    "Hier Hinweise für Github\n",
    "https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/\n",
    "\n",
    "Diese Referenz: <blockquote>git+https://github.com/django/django.git@45dfb3641aa4d9828a7c5448d11aa67c7cbd7966</blockquote>\n",
    "in einem requirements.txt lässt sich mit \n",
    "pip install -r ebenfalls installieren.\n",
    "\n",
    "Bin bei der Abwägung der Vor- und Nachteile noch nicht entschieden. Hier nur als Vorschlag.\n",
    "\n",
    "Frage: Ist es möglich, innerhalb eines Notebooks \n",
    "import results_saving_funcs as rsf\n",
    "durch eine Tastenkombination direkt nach results_savin_funcs.py zu springen?\n",
    "\n",
    "3. **Aufruf der notbooks in einer Helper function**  \n",
    "\n",
    "in **results_savings_funcs.py** nutzt Du die Funktion **run_notebooks** um die Notebooks auszuführen\n",
    "\n",
    "Hier wiederum nutzt Du Funktionen von bestimmten libraries\n",
    "\n",
    "```python\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import nbparameterise as nbp\n",
    "```\n",
    "\n",
    "Ich finde in den Notebooks nicht die Parameter, die Du mit dem nachfolgenden Aufruf ansprichst.  \n",
    "- Kannst Du mir Hinweise geben, wo ich das finde?\n",
    "\n",
    "=> AJ : Immer und zwingend und ausschliesslich in der jeweils ersten Zeile IN\\[1\\]\n",
    "\n",
    "- diese Aufrufform nutzt Du für jeden 'Run'. Ist es egal, wenn in einem Notebook ein solcher parameter nicht vorhanden ist oder sind diese immer vorhanden.\n",
    "\n",
    "=> AJ : Ja, ist egal. Das Notebook muss nur jene deklarieren, die es geändert haben will beim Aufruf aus Notebook 0. Alle anderen bleiben wie sie mitgegeben werden.\n",
    "\n",
    "\n",
    "```python\n",
    "params = nbp.parameter_values(orig_parameters,\n",
    "                          execution_mode=runtime_param_dict['em'],\n",
    "                          oversampling=runtime_param_dict['os'],\n",
    "                          modification_ratio = runtime_param_dict['mr'],\n",
    "                          sampling_fraction_nreb = runtime_param_dict['dsn'],\n",
    "                          sampling_fraction_reb = runtime_param_dict['dsw'],\n",
    "                          factor=runtime_param_dict['fa'],\n",
    "                          exactDate_mode = runtime_param_dict['me'],\n",
    "                          strip_number_digits = runtime_param_dict['sn']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ich bekomme einen Fehler beim Versuch die Notebooks laufen zu lassen:**\n",
    "\n",
    "```python\n",
    "\n",
    "Run id 0\n",
    "Executing notebook 1_DataAnalysis.ipynb\n",
    "Executing notebook 2_GoldstandardDataPreparation.ipynb\n",
    "Executing notebook 3_DataSynthesizing.ipynb\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "CellExecutionError                        Traceback (most recent call last)\n",
    "/tmp/ipykernel_8207/2797365603.py in <module>\n",
    "     11 for run in range(len(runtime_param_dict_list)):\n",
    "     12     print('\\nRun id', run)\n",
    "---> 13     rsf.run_notebooks(notebook, runtime_param_dict_list[run], run, path_results)\n",
    "     14 \n",
    "     15     # Save the resulting handover files for the run done right now\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/results_saving_funcs.py in run_notebooks(notebook_list, runtime_param_dict, run, path)\n",
    "    116             ep = ExecutePreprocessor(timeout=None)\n",
    "    117             # ... and execute it.\n",
    "--> 118             ep.preprocess(nb, {\"metadata\": {\"path\": './'}})\n",
    "    119         # Save notebook run in result file\n",
    "    120         save_notebook_results(nb, path, notebook_list[i][:-6] + '_run_' + str(run) + '.ipynb')\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py in preprocess(self, nb, resources, km)\n",
    "     82             self.nb.metadata['language_info'] = info_msg['content']['language_info']\n",
    "     83             for index, cell in enumerate(self.nb.cells):\n",
    "---> 84                 self.preprocess_cell(cell, resources, index)\n",
    "     85         self.set_widgets_metadata()\n",
    "     86 \n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py in preprocess_cell(self, cell, resources, index)\n",
    "    103         \"\"\"\n",
    "    104         self._check_assign_resources(resources)\n",
    "--> 105         cell = self.execute_cell(cell, index, store_history=True)\n",
    "    106         return cell, self.resources\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nbclient/util.py in wrapped(*args, **kwargs)\n",
    "     72     \"\"\"\n",
    "     73     def wrapped(*args, **kwargs):\n",
    "---> 74         return just_run(coro(*args, **kwargs))\n",
    "     75     wrapped.__doc__ = coro.__doc__\n",
    "     76     return wrapped\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nbclient/util.py in just_run(coro)\n",
    "     51         nest_asyncio.apply()\n",
    "     52         check_patch_tornado()\n",
    "---> 53     return loop.run_until_complete(coro)\n",
    "     54 \n",
    "     55 \n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nest_asyncio.py in run_until_complete(self, future)\n",
    "     68                 raise RuntimeError(\n",
    "     69                     'Event loop stopped before Future completed.')\n",
    "---> 70             return f.result()\n",
    "     71 \n",
    "     72     def _run_once(self):\n",
    "\n",
    "~/Downloads/python/bin_3_7_11/lib/python3.7/asyncio/futures.py in result(self)\n",
    "    179         self.__log_traceback = False\n",
    "    180         if self._exception is not None:\n",
    "--> 181             raise self._exception\n",
    "    182         return self._result\n",
    "    183 \n",
    "\n",
    "~/Downloads/python/bin_3_7_11/lib/python3.7/asyncio/tasks.py in __step(***failed resolving arguments***)\n",
    "    247                 # We use the `send` method directly, because coroutines\n",
    "    248                 # don't have `__iter__` and `__next__` methods.\n",
    "--> 249                 result = coro.send(None)\n",
    "    250             else:\n",
    "    251                 result = coro.throw(exc)\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nbclient/client.py in async_execute_cell(self, cell, cell_index, execution_count, store_history)\n",
    "    855         if execution_count:\n",
    "    856             cell['execution_count'] = execution_count\n",
    "--> 857         self._check_raise_for_error(cell, exec_reply)\n",
    "    858         self.nb['cells'][cell_index] = cell\n",
    "    859         return cell\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/nbclient/client.py in _check_raise_for_error(self, cell, exec_reply)\n",
    "    758 \n",
    "    759         if not cell_allows_errors:\n",
    "--> 760             raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
    "    761 \n",
    "    762     async def async_execute_cell(\n",
    "\n",
    "CellExecutionError: An error occurred while executing the following cell:\n",
    "------------------\n",
    "import data_preparation_funcs as dpf\n",
    "\n",
    "for i in ['original', 'modified']:\n",
    "    goldstandard_uniques[i] = dpf.attribute_preprocessing(\n",
    "        goldstandard_uniques[i],\n",
    "        columns_metadata_dict['data_analysis_columns'], strip_number_digits)\n",
    "------------------\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "/tmp/ipykernel_8335/718141576.py in <module>\n",
    "      4     goldstandard_uniques[i] = dpf.attribute_preprocessing(\n",
    "      5         goldstandard_uniques[i],\n",
    "----> 6         columns_metadata_dict['data_analysis_columns'], strip_number_digits)\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/data_preparation_funcs.py in attribute_preprocessing(df, columns, strip_digits)\n",
    "    173             continue # Explicitly : do nothing!\n",
    "    174         elif attrib in ['coordinate_E']:\n",
    "--> 175             df = split_coordinate(df)\n",
    "    176         elif attrib in ['corporate_full']:\n",
    "    177             df = split_dictionary_column(df, 'corporate', ['110', '710'#, '810\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/data_preparation_funcs.py in split_coordinate(df)\n",
    "    144 def split_coordinate (df) :\n",
    "    145     df['coordinate_E'] = df['coordinate']\n",
    "--> 146     df = norm_first_coordinate(df, '_E')\n",
    "    147     # Recuce list to N and S and then same procedure\n",
    "    148     df['coordinate_N'] = df['coordinate']\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/data_preparation_funcs.py in norm_first_coordinate(df, suffix)\n",
    "    129 \n",
    "    130 def norm_first_coordinate (df, suffix) :\n",
    "--> 131     df['coordinate'+suffix] = df['coordinate'+suffix].map(lambda x : x[0] if len(x)>0 else '').str.replace(' ', '').str.replace('.', '').str[:8].str.lower()\n",
    "    132 \n",
    "    133     return df\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/pandas/core/series.py in map(self, arg, na_action)\n",
    "   3826         dtype: object\n",
    "   3827         \"\"\"\n",
    "-> 3828         new_values = super()._map_values(arg, na_action=na_action)\n",
    "   3829         return self._constructor(new_values, index=self.index).__finalize__(self)\n",
    "   3830 \n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/venv/lib/python3.7/site-packages/pandas/core/base.py in _map_values(self, mapper, na_action)\n",
    "   1298 \n",
    "   1299         # mapper is a function\n",
    "-> 1300         new_values = map_f(values, mapper)\n",
    "   1301 \n",
    "   1302         return new_values\n",
    "\n",
    "pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()\n",
    "\n",
    "~/environment/code/learning/gh_scala/clustering_metadata/data_preparation_funcs.py in <lambda>(x)\n",
    "    129 \n",
    "    130 def norm_first_coordinate (df, suffix) :\n",
    "--> 131     df['coordinate'+suffix] = df['coordinate'+suffix].map(lambda x : x[0] if len(x)>0 else '').str.replace(' ', '').str.replace('.', '').str[:8].str.lower()\n",
    "    132 \n",
    "    133     return df\n",
    "\n",
    "TypeError: object of type 'float' has no len()\n",
    "TypeError: object of type 'float' has no len()\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "@Andreas: kannst Du das nachvollziehen?\n",
    "\n",
    "=> AJ : Ja, der gleiche Fehler. Interessant! Das muss wohl mit den neuen Trainings-Daten zusammenhängen. Die Datensynthetisierung ist nicht wichtig. Du kannst generell diesen Schritt auskommentieren. Ich habe am 01.08. tatsächlich strikt ohne Datensynthetisierung laufen lassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"play around\" nach dem Fehler**\n",
    "\n",
    "habe dann noch ein wenig herumgespielt. Hier das bisschen python code\n",
    "\n",
    "```python\n",
    "import results_saving_funcs as rsf\n",
    "\n",
    "\n",
    "run = 0\n",
    "\n",
    "path_results = './results'\n",
    "\n",
    "\n",
    "results = rsf.restore_dict_results(path_results, 'results_run_' + str(run) + '.pkl')\n",
    "\n",
    "print(results.keys())\n",
    "\n",
    "print(type(results.get('results_best_model')))\n",
    "\n",
    "print()\n",
    "\n",
    "print(results.get('results_best_model'))\n",
    "\n",
    "```\n",
    "\n",
    "mit diesem Ergebnis:\n",
    "\n",
    "```bash\n",
    "dict_keys(['results_best_model', 'results_model_scores'])\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "\n",
    "                       model        auc   accuracy  precision     recall  \\\n",
    "0            DummyClassifier  50.152071  97.243323   1.712329   1.694915   \n",
    "0     DecisionTreeClassifier  96.927346  99.871005  96.853147  93.898305   \n",
    "0  DecisionTreeClassifier_CV  97.762689  99.871005  95.270270  95.593220   \n",
    "0     RandomForestClassifier  97.268752  99.885338  97.212544  94.576271   \n",
    "0                        SVC  96.225150  99.804118  93.493151  92.542373   \n",
    "0                     SVC_CV  97.072608  99.828006  93.602694  94.237288   \n",
    "0              NeuralNetwork  96.573825  99.832784  94.827586  93.220339   \n",
    "\n",
    "    auc_log  accuracy_log  precision_log  recall_log  \n",
    "0  0.696193      3.591144       0.017272    0.017094  \n",
    "0  3.482628      6.653150       3.458767    2.796604  \n",
    "0  3.799895      6.653150       3.051302    3.122026  \n",
    "0  3.600412      6.770933       3.580041    2.914387  \n",
    "0  3.276810      6.235415       2.732315    2.595933  \n",
    "0  3.531058      6.365468       2.749293    2.853762  \n",
    "0  3.373726      6.393639       2.961831    2.691243  \n",
    "\n",
    "```\n",
    "\n",
    "Ok, Du serialisierst als DataFrame-type. Kenne ich noch zu wenig, stelle ich mir als grosses Excel sheet vor.\n",
    "\n",
    "=> AJ : Das ist richtig. Vgl. jeweils die Abschnitte \"Metadata Handover\" immer ganz am Ende eines Notebooks! Nach der pikle library kannst du googlen.\n",
    "\n",
    "Was bedeuten\n",
    "\n",
    "- Unterschied auc/accuracy\n",
    "\n",
    "=> AJ : AUC steht für \"Area under the curve\". Die Accuracy und die AUC definiere ich in Notebook 6 Decision Tree Model und dort in Abschnitt <a href='../6_DecisionTreeModel.ipynb#Performance-Measurement-of-Decision-Tree#Performance-Measurement-of-Decision-Tree'>Performance Measurement of Decision Tree</a>.\n",
    "\n",
    "- wie sind die Werte der Spalten auc_log, accuracy_log, precision_log, recall_log zu interpretieren?\n",
    "\n",
    "=> AJ : Jeweils als log(auc), log(accuracy), log(precision), log(recall). Wenn du Werte von 99.9x% hast, sieht alles gleich (super) aus. Die Mathematiker logarithmieren alles, um auf grosse Werte bei den Unterschieden zu kommen. Ein kleiner Manipulationstrick, um mehr zu sehen.\n",
    "\n",
    "results_model_score ist durch den Fehler ohne Werte (None)\n",
    "\n",
    "=> AJ : Verstehe nicht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

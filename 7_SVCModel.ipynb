{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_mode = 'restricted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the Ensemble model family, a Support Vector Classifier shall be calculated in this chapter. The Support Vector Classifier will be modelled in two modes. The first mode will be without cross-validation, while the second mode will be with cross-validation. The structure of this chapter will be identical to the structure drawn in the past chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Data Takeover](#Data-Takeover)\n",
    "- [Support Vector Classifier](#Support-Vector-Classifier)\n",
    "    - [Train/Test Split for Support Vector Classifier](#Train/Test-Split-for-Support-Vector-Classifier)\n",
    "    - [Model Training for Support Vector Classifier](#Model-Training-for-Support-Vector-Classifier)\n",
    "    - [Performance Measurement for Support Vector Classifier](#Performance-Measurement-for-Support-Vector-Classifier)\n",
    "- [Support Vector Classifier with Cross-Validation](#Support-Vector-Classifier-with-Cross-Validation)\n",
    "    - [Train/Test Split for Support Vector Classifier CV](#Train/Test-Split-for-Support-Vector-Classifier-CV)\n",
    "    - [Model Training for Support Vector Classifier CV](#Model-Training-for-Support-Vector-Classifier-CV)\n",
    "    - [Performance Measurement of Support Vector Classifier CV](#Performance-Measurement-of-Support-Vector-Classifier-CV)\n",
    "- [Summary](#Summary)\n",
    "    - [Results Handover](#Results-Handover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Takeover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from chapters [Feature Matrix Generation](./4_FeatureMatrixGeneration.ipynb) and [Features Discussion and Dummy Classifier Baseline](./5_FeatureDiscussionDummyBaseline.ipynb) have to be read in as input for processing in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coordinate_E_delta</th>\n",
       "      <th>coordinate_N_delta</th>\n",
       "      <th>corporate_full_delta</th>\n",
       "      <th>doi_delta</th>\n",
       "      <th>edition_delta</th>\n",
       "      <th>exactDate_delta</th>\n",
       "      <th>format_prefix_delta</th>\n",
       "      <th>format_postfix_delta</th>\n",
       "      <th>isbn_delta</th>\n",
       "      <th>ismn_delta</th>\n",
       "      <th>...</th>\n",
       "      <th>part_delta</th>\n",
       "      <th>person_100_delta</th>\n",
       "      <th>person_700_delta</th>\n",
       "      <th>person_245c_delta</th>\n",
       "      <th>pubinit_delta</th>\n",
       "      <th>scale_delta</th>\n",
       "      <th>ttlfull_245_delta</th>\n",
       "      <th>ttlfull_246_delta</th>\n",
       "      <th>volumes_delta</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.697740</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.818905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   coordinate_E_delta  coordinate_N_delta  corporate_full_delta  doi_delta  \\\n",
       "0                -0.1                -0.1                  -0.1       -0.1   \n",
       "1                -0.1                -0.1                  -0.1       -0.1   \n",
       "2                -0.1                -0.1                  -0.1       -0.1   \n",
       "3                -0.1                -0.1                  -0.1       -0.1   \n",
       "4                -0.1                -0.1                  -0.1       -0.1   \n",
       "\n",
       "   edition_delta  exactDate_delta  format_prefix_delta  format_postfix_delta  \\\n",
       "0           -0.1             0.75                  1.0                   1.0   \n",
       "1           -0.1             0.75                  1.0                   1.0   \n",
       "2           -0.1             0.75                  1.0                   1.0   \n",
       "3           -0.1             0.75                  1.0                   1.0   \n",
       "4           -0.1             0.75                  1.0                   1.0   \n",
       "\n",
       "   isbn_delta  ismn_delta  ...  part_delta  person_100_delta  \\\n",
       "0         1.0        -0.1  ...         1.0               1.0   \n",
       "1         1.0        -0.1  ...         1.0               1.0   \n",
       "2         1.0        -0.1  ...         1.0               1.0   \n",
       "3         1.0        -0.1  ...         1.0               1.0   \n",
       "4         1.0        -0.1  ...         1.0               1.0   \n",
       "\n",
       "   person_700_delta  person_245c_delta  pubinit_delta  scale_delta  \\\n",
       "0              1.00           1.000000       1.000000         -0.1   \n",
       "1             -0.05           0.818905       0.848485         -0.1   \n",
       "2             -0.05           0.697740       0.848485         -0.1   \n",
       "3             -0.05           0.818905       0.848485         -0.1   \n",
       "4             -0.10           1.000000       1.000000         -0.1   \n",
       "\n",
       "   ttlfull_245_delta  ttlfull_246_delta  volumes_delta  duplicates  \n",
       "0           1.000000               -0.1            1.0           1  \n",
       "1           0.787879               -0.1            1.0           1  \n",
       "2           1.000000               -0.1            1.0           1  \n",
       "3           0.787879               -0.1            1.0           1  \n",
       "4           1.000000               -0.1            1.0           1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_goldstandard = './daten_goldstandard'\n",
    "\n",
    "# Restore results so far\n",
    "df_labelled_feature_matrix = pd.read_pickle(os.path.join(path_goldstandard,\n",
    "                                                         'labelled_feature_matrix.pkl'),\n",
    "                                 compression=None)\n",
    "\n",
    "df_attribute_with_sim_feature = pd.read_pickle(os.path.join(\n",
    "    path_goldstandard, 'labelled_feature_matrix_full.pkl'), compression=None\n",
    "                                              )\n",
    "\n",
    "df_labelled_feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of duplicates (1) on uniques (0) in units of [%]\n",
      "0    99.43\n",
      "1     0.57\n",
      "Name: duplicates, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Part of duplicates (1) on uniques (0) in units of [%]')\n",
    "print(round(df_labelled_feature_matrix.duplicates.value_counts(normalize=True)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine allows the use of regularization. To get an idea on its basic behaiour, a first model with grid search but without cross-validation will be calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split for Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three data sets like for training, validation and testing have to be generated by splitting the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.1       , -0.1       , -0.1       , -0.05      , -0.1       ,\n",
       "          0.625     ,  0.        ,  0.42857143,  1.        , -0.1       ,\n",
       "         -0.05      , -0.05      ,  0.49267677, -0.05      ,  0.54033531,\n",
       "         -0.05      , -0.1       ,  0.57608486, -0.1       , -0.05      ],\n",
       "        [-0.1       , -0.1       , -0.1       , -0.1       , -0.05      ,\n",
       "          0.5       ,  0.        ,  0.42857143,  0.        , -0.1       ,\n",
       "         -0.1       ,  0.        , -0.05      , -0.05      ,  0.50978836,\n",
       "         -0.05      , -0.1       ,  0.56688312, -0.1       ,  0.51111111],\n",
       "        [-0.05      , -0.05      ,  0.06      , -0.1       , -0.1       ,\n",
       "          0.5       ,  0.        ,  0.42857143,  0.        , -0.1       ,\n",
       "         -0.1       , -0.05      , -0.1       , -0.1       , -0.05      ,\n",
       "         -0.05      , -0.05      ,  0.46245348, -0.05      , -0.05      ],\n",
       "        [-0.1       , -0.1       , -0.05      , -0.1       , -0.1       ,\n",
       "          0.625     ,  0.        ,  0.11111111,  1.        , -0.1       ,\n",
       "         -0.05      , -0.1       , -0.05      , -0.05      ,  0.4047619 ,\n",
       "          0.48095238, -0.1       ,  0.4667789 , -0.1       ,  0.55555556],\n",
       "        [-0.1       , -0.1       ,  0.05      , -0.1       , -0.05      ,\n",
       "          0.25      ,  0.        ,  0.11111111,  0.        , -0.1       ,\n",
       "         -0.05      , -0.1       , -0.05      ,  0.47484737,  0.4973368 ,\n",
       "          0.56635908, -0.1       ,  0.55436185, -0.1       ,  0.77777778]]),\n",
       " array([0, 0, 0, 0, 0]),\n",
       " array([153040,  72045, 177429, 149431, 170753]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import classifier_fitting_funcs as cff\n",
    "\n",
    "X_tr, X_val, X_te, y_tr, y_val, y_te, idx_tr, idx_val, idx_te = cff.split_feature_target(\n",
    "    df_labelled_feature_matrix, 'train_validation_test')\n",
    "\n",
    "X_tr[:5], y_tr[:5], idx_tr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166033, 20) (166033,) (41509, 20) (41509,) (51886, 20) (51886,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape, X_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training for Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space for grid search is specific for Support Vector Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grid parameters are ...\n",
      "kernel ['poly']\n",
      "degree [3]\n",
      "gamma [2.0]\n",
      "C [0.5]\n",
      "class_weight [None]\n",
      " => Number of combinations : 1\n"
     ]
    }
   ],
   "source": [
    "if execution_mode == 'full' :\n",
    "    # Kernel 'rbf' has long calculation times, but does not generate\n",
    "    #  the best accuracy : Ommit in grid search.\n",
    "    parameter_dictionary = {\n",
    "        'kernel' : ['linear', 'poly'],\n",
    "        'degree' : [2, 3, 4],\n",
    "        'gamma' : [1.5, 2, 2.5, 'auto'],\n",
    "        'C' : [0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "        'class_weight' : [None, 'balanced']\n",
    "    }\n",
    "elif execution_mode == 'restricted' :\n",
    "    parameter_dictionary = {\n",
    "        'kernel' : ['poly'],\n",
    "        'degree' : [3],\n",
    "        'gamma' : [2.0],\n",
    "        'C' : [0.5],\n",
    "        'class_weight' : [None]\n",
    "    }\n",
    "\n",
    "# Grid of values\n",
    "grid = cff.generate_parameter_grid(parameter_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifiers are fitted on each point of the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting with parameters {'C': 0.5, 'class_weight': None, 'degree': 3, 'gamma': 2.0, 'kernel': 'poly'}\n",
      " => validation score 99.889%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "sv = SVC(random_state=0)\n",
    "\n",
    "# Save accuracy on test set\n",
    "test_scores = []\n",
    "for params_dict in grid :\n",
    "    test_scores.append(cff.fit_model_measure_scores(sv, params_dict, X_tr, y_tr, X_val, y_val))\n",
    "\n",
    "# Save measured accuracies\n",
    "df_test_scores_sv = pd.DataFrame(test_scores).sort_values('accuracy_val', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for the best estimator of Support Vector Classifier without cross-validation are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters for the best model are ...\n",
      "kernel = poly\n",
      "degree = 3\n",
      "gamma = 2.0\n",
      "C = 0.5\n",
      "class_weight = None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=2.0, kernel='poly',\n",
       "    max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = cff.get_best_parameters(test_scores, parameter_dictionary)\n",
    "\n",
    "# Create a decision tree\n",
    "sv_best = SVC(gamma=best_params['gamma'], kernel=best_params['kernel'],\n",
    "              C=best_params['C'], class_weight=best_params['class_weight'],\n",
    "              degree=best_params['degree'], random_state=0\n",
    "             )\n",
    "\n",
    "# Fit estimator\n",
    "sv_best.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement for Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance measurement of the best estimator is done as described in chapter [Decision Tree Model](./6_DecisionTreeModel.ipynb) and is shown without any further comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51563,    28],\n",
       "       [   29,   266]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_sv = sv_best.predict(X_te)\n",
    "\n",
    "confusion_matrix(y_te, y_pred_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 99.890%\n",
      "Area under the curve 95.058% - accuracy 99.890% - precision 90.476% - recall 90.169%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "print('Score {:.3f}%'.format(100*sv_best.score(X_te, y_te)))\n",
    "print('Area under the curve {:.3f}% - accuracy {:.3f}% - precision {:.3f}% - recall {:.3f}%'.format(100*roc_auc_score(y_te, y_pred_sv),\n",
    "                100*accuracy_score(y_te, y_pred_sv),\n",
    "                100*precision_score(y_te, y_pred_sv),\n",
    "                100*recall_score(y_te, y_pred_sv)\n",
    "               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrongly classified records of this classifier have to be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import results_analysis_funcs as raf\n",
    "import results_saving_funcs as rsf\n",
    "\n",
    "idx = {}\n",
    "idx['true_predicted_uniques'], idx['true_predicted_duplicates'], idx['false_predicted_uniques'], idx['false_predicted_duplicates'] = raf.get_confusion_matrix_indices(y_te, y_pred_sv)\n",
    "\n",
    "wrong_prediction_groups = ['false_predicted_uniques', 'false_predicted_duplicates']\n",
    "\n",
    "for i in wrong_prediction_groups :\n",
    "    rsf.add_wrong_predictions(path_goldstandard, \n",
    "                              sv_best, i, df_attribute_with_sim_feature.iloc[idx_te].iloc[idx[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To harden the result above, another Support Vector Classifier shall be fitted, now with the use of cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split for Support Vector CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation with object $\\texttt{GridSearchCV}$ from scikit-learn splits the training data into training and validation parts. It is sufficient to split the original data into a train and a test data set without any additional splitting of the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.1       , -0.1       , -0.05      , -0.1       , -0.1       ,\n",
       "          0.25      ,  0.        ,  0.42857143,  0.        , -0.1       ,\n",
       "          0.16666667, -0.1       , -0.05      , -0.05      ,  0.53888889,\n",
       "          0.47991021, -0.1       ,  0.59978811, -0.1       ,  0.78333333],\n",
       "        [-0.1       , -0.1       , -0.1       , -0.1       , -0.1       ,\n",
       "          0.4375    ,  0.        ,  0.11111111,  1.        , -0.1       ,\n",
       "         -0.05      , -0.1       ,  1.        ,  0.57605284,  0.59184563,\n",
       "          0.41919192, -0.1       ,  0.7332472 , -0.1       ,  0.        ],\n",
       "        [-0.1       , -0.1       ,  0.05      , -0.1       , -0.1       ,\n",
       "          0.25      ,  1.        ,  1.        ,  1.        , -0.1       ,\n",
       "         -0.1       , -0.1       , -0.05      ,  0.52608873,  0.61453149,\n",
       "          0.41568627, -0.1       ,  0.51855227, -0.1       ,  0.        ],\n",
       "        [-0.1       , -0.1       , -0.1       , -0.1       , -0.1       ,\n",
       "          0.5       ,  1.        ,  0.42857143,  0.        , -0.1       ,\n",
       "         -0.1       ,  0.61111111,  0.55357143, -0.05      ,  0.49804219,\n",
       "         -0.05      , -0.1       ,  0.64228804, -0.1       ,  0.51111111],\n",
       "        [-0.1       , -0.1       , -0.1       , -0.1       , -0.1       ,\n",
       "          0.25      ,  1.        ,  0.42857143,  0.        , -0.1       ,\n",
       "         -0.1       ,  0.        , -0.1       , -0.05      ,  0.50943557,\n",
       "          0.45171958, -0.1       ,  0.6121175 , -0.1       ,  0.        ]]),\n",
       " array([0, 0, 0, 0, 0]),\n",
       " array([  7686, 251455, 121736,  30480, 184004]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, _, X_te, y_tr, _, y_te, idx_tr, _, idx_te = cff.split_feature_target(\n",
    "    df_labelled_feature_matrix, 'train_test')\n",
    "\n",
    "X_tr[:5], y_tr[:5], idx_tr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207542, 20) (207542,) (51886, 20) (51886,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape, y_tr.shape, X_te.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training for Support Vector CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter grid remains the same as with the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Create cross-validation object with DecisionTreeClassifer\n",
    "grid_cv = GridSearchCV(SVC(random_state=0),\n",
    "                       param_grid = parameter_dictionary, cv=5\n",
    "                       , verbose=1\n",
    "                      )\n",
    "\n",
    "# Fit estimator\n",
    "grid_cv.fit(X_tr, y_tr)\n",
    "\n",
    "# Get the results with 'cv_results_', get parameters with their scores\n",
    "params = pd.DataFrame(grid_cv.cv_results_['params'])\n",
    "scores = pd.DataFrame(grid_cv.cv_results_['mean_test_score'], columns=['accuracy_val'])\n",
    "log_scores = pd.DataFrame(np.log(1-grid_cv.cv_results_['mean_test_score']), columns=['log_accuracy_val'])\n",
    "scores_std = pd.DataFrame(grid_cv.cv_results_['std_test_score'], columns=['std_accuracy_val'])\n",
    "\n",
    "# Create a DataFrame of (parameters, score, std) pairs\n",
    "df_test_scores_svcv = params.merge(scores, how='inner', left_index=True, right_index=True)\n",
    "df_test_scores_svcv = df_test_scores_svcv.merge(\n",
    "    scores_std, how='inner', left_index=True, right_index=True).sort_values(\n",
    "    'accuracy_val', ascending=False)\n",
    "df_test_scores_svcv = df_test_scores_svcv.merge(\n",
    "    log_scores, how='inner', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>degree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>std_accuracy_val</th>\n",
       "      <th>log_accuracy_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.999056</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-6.964974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C class_weight  degree  gamma kernel  accuracy_val  std_accuracy_val  \\\n",
       "0  0.5         None       3    2.0   poly      0.999056          0.000169   \n",
       "\n",
       "   log_accuracy_val  \n",
       "0         -6.964974  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_scores_svcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=2.0, kernel='poly',\n",
       "    max_iter=-1, probability=False, random_state=0, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcv_best = grid_cv.best_estimator_\n",
    "svcv_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measurement of Support Vector Classifier CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of this subsection has been described previously and remains the same like for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51559,    32],\n",
       "       [   25,   270]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_svcv = svcv_best.predict(X_te)\n",
    "\n",
    "confusion_matrix(y_te, y_pred_svcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 99.890%\n",
      "Area under the curve 95.732% - accuracy 99.890% - precision 89.404% - recall 91.525%\n"
     ]
    }
   ],
   "source": [
    "print('Score {:.3f}%'.format(100*svcv_best.score(X_te, y_te)))\n",
    "print('Area under the curve {:.3f}% - accuracy {:.3f}% - precision {:.3f}% - recall {:.3f}%'.format(\n",
    "    100*roc_auc_score(y_te, y_pred_svcv),\n",
    "                100*accuracy_score(y_te, y_pred_svcv),\n",
    "                100*precision_score(y_te, y_pred_svcv),\n",
    "                100*recall_score(y_te, y_pred_svcv)\n",
    "               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For results comparison, the wrongly classified records need to be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = {}\n",
    "idx['true_predicted_uniques'], idx['true_predicted_duplicates'], idx['false_predicted_uniques'], idx['false_predicted_duplicates'] = raf.get_confusion_matrix_indices(y_te, y_pred_svcv)\n",
    "\n",
    "wrong_prediction_groups = ['false_predicted_uniques', 'false_predicted_duplicates']\n",
    "\n",
    "for i in wrong_prediction_groups :\n",
    "    rsf.add_wrong_predictions(path_goldstandard, \n",
    "                              svcv_best, i, df_attribute_with_sim_feature.iloc[idx_te].iloc[idx[i]], '_CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter shows the modelling and the performance of Support Vector Classifiers with regularization. The final comparison of the results of this chapter with the results of all other models will be done in [Overview and Summary](./0_OverviewSummary.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Handover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a consolidated view of all results, the performance of both Support Vector Classifiers of this chapter have to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf.add_result_to_results(path_goldstandard,\n",
    "                          df_test_scores_sv, sv_best, X_te, y_te, y_pred_sv)\n",
    "rsf.add_result_to_results(path_goldstandard, \n",
    "                          df_test_scores_svcv, svcv_best, X_te, y_te, y_pred_svcv, '_CV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
